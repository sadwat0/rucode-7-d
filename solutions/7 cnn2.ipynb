{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTERS = \"абвгдежзийклмнопрстуфхцчшщъыьэюя\"\n",
    "VOWELS = \"аеиоуэюяы\"\n",
    "N_LETTERS = len(LETTERS)\n",
    "MAX_WORD_LENGTH = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['аа^к', 'аа^ка', 'аа^ке', 'аа^ки', 'аа^ков'],\n",
       " 495,\n",
       " ['абдоминомедиастинальный',\n",
       "  'абдоминоперикардиостомией',\n",
       "  'абдоминоперикардиостомиею',\n",
       "  'абдоминоперикардиостомии',\n",
       "  'абдоминоперикардиостомия'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = open('data/train_stresses_labels.txt', 'r', encoding=\"utf8\").readlines()\n",
    "train_word = []\n",
    "x = []\n",
    "for i in range(len(train)):\n",
    "    train[i] = train[i][:-1]\n",
    "    if len(train[i]) <= MAX_WORD_LENGTH:\n",
    "        train_word.append(train[i].replace('^', '').replace('ё', 'е'))\n",
    "    else:\n",
    "        x.append(train[i].replace('^', '').replace('ё', 'е'))\n",
    "train[:5], len(x), x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "аакам ааками ааленец аама аамами аамов аамом аамы аангичами аангичах\n"
     ]
    }
   ],
   "source": [
    "test = open('data/public_test_stresses.txt', 'r', encoding=\"utf8\").readlines()\n",
    "for i in range(len(test)):\n",
    "    test[i] = test[i][:-1].replace('ё', '')\n",
    "print(*test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_to_index(letter):\n",
    "    return LETTERS.find(letter)\n",
    "\n",
    "def letter_to_tensor(letter):\n",
    "    tensor = torch.zeros(1, N_LETTERS)\n",
    "    tensor[0][letter_to_index(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# <1 x max_word_length x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def word_to_tensor(word):\n",
    "    tensor = torch.zeros(1, MAX_WORD_LENGTH, N_LETTERS)\n",
    "    for i, letter in enumerate(word):\n",
    "        tensor[0][i][letter_to_index(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 587995/587995 [01:56<00:00, 5048.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = [], []\n",
    "for i in tqdm(range(len(train_word))):\n",
    "    if len(train[i]) <= MAX_WORD_LENGTH:\n",
    "        X.append(word_to_tensor(train_word[i]))\n",
    "        correct_pos = train[i].find('^')\n",
    "        y.append(torch.zeros(MAX_WORD_LENGTH))\n",
    "        y[-1][correct_pos] = 1\n",
    "X = torch.stack(X)\n",
    "y = torch.stack(y)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_tensor, target_tensor):\n",
    "        self.data = data_tensor\n",
    "        self.target = target_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.target[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = CustomDataset(X_train, y_train)\n",
    "val_files = CustomDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # 1 x 23 x 32\n",
    "            nn.Conv2d(1, 8, kernel_size=(3, N_LETTERS)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            \n",
    "            # 8 x 21 x 1\n",
    "            nn.Conv2d(8, 16, kernel_size=(3, 1), padding=(0, 1)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # 16 x 9 x 1\n",
    "            nn.Conv2d(16, 32, kernel_size=(3, 1), padding=(0, 1)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # 32 x 7 x 12\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 1), padding=(0, 1)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # 64 x 5 x 10\n",
    "            nn.Conv2d(64, 64, kernel_size=(3, 1), padding=(0, 1)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # 64 x 3 x 8\n",
    "            nn.Conv2d(64, 32, kernel_size=(3, 1), padding=(0, 1)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(0.3)\n",
    "            \n",
    "            # 32 x 2 x 7\n",
    "        )\n",
    "        \n",
    "        self.fully_conected = nn.Sequential(\n",
    "            nn.Linear(288, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            # nn.Dropout(0.25),\n",
    "            nn.BatchNorm1d(256),\n",
    "            \n",
    "            nn.Linear(256, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.BatchNorm1d(256),\n",
    "            \n",
    "            # nn.Linear(512, 128),\n",
    "            # nn.LeakyReLU(),\n",
    "            # # nn.Dropout(0.25),\n",
    "            # nn.BatchNorm1d(128),\n",
    "            \n",
    "            # nn.Linear(128, 64),\n",
    "            # nn.LeakyReLU(),\n",
    "            # # nn.Dropout(0.25),\n",
    "            # nn.BatchNorm1d(64),\n",
    "            \n",
    "            nn.Linear(256, MAX_WORD_LENGTH),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        # print(x.shape)\n",
    "        \n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.fully_conected(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_epoch(model, train_loader, criterion, optimizer):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_data = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        # print(outputs.shape, labels[0])\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == torch.argmax(labels.data, 1))\n",
    "        processed_data += inputs.size(0)\n",
    "              \n",
    "    train_loss = running_loss / processed_data\n",
    "    train_acc = running_corrects.cpu().numpy() / processed_data\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_size = 0\n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == torch.argmax(labels.data))\n",
    "        processed_size += inputs.size(0)\n",
    "    val_loss = running_loss / processed_size\n",
    "    val_acc = running_corrects.double() / processed_size\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, batch_size):\n",
    "    train_loader = DataLoader(train_files, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_files, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
    "        val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
    "    \n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "        # opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        opt = torch.optim.AdamW(model.parameters())\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=opt, gamma=0.9)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n",
    "            print(\"loss\", train_loss)\n",
    "            \n",
    "            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
    "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
    "            \n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
    "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 174623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|          | 0/20 [00:00<?, ?it/s]c:\\Users\\Ivan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 3.035331511752309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   5%|▌         | 1/20 [01:35<30:07, 95.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 001 train_loss: 3.0353         val_loss 3.0051 train_acc 0.1653 val_acc 0.1507\n",
      "loss 3.0028662674864357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  10%|█         | 2/20 [02:58<26:30, 88.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 002 train_loss: 3.0029         val_loss 3.0563 train_acc 0.2023 val_acc 0.0922\n",
      "loss 2.9798568596958446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  15%|█▌        | 3/20 [04:24<24:38, 86.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 003 train_loss: 2.9799         val_loss 2.9695 train_acc 0.2254 val_acc 0.1440\n",
      "loss 2.96442479779755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  20%|██        | 4/20 [05:48<22:53, 85.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 004 train_loss: 2.9644         val_loss 2.9620 train_acc 0.2406 val_acc 0.1362\n",
      "loss 2.956842594754175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  25%|██▌       | 5/20 [07:11<21:15, 85.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 005 train_loss: 2.9568         val_loss 2.9587 train_acc 0.2487 val_acc 0.1373\n",
      "loss 2.9475669289263204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  30%|███       | 6/20 [08:37<19:53, 85.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 006 train_loss: 2.9476         val_loss 2.9481 train_acc 0.2583 val_acc 0.1289\n",
      "loss 2.9349627736231514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  35%|███▌      | 7/20 [10:02<18:29, 85.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 007 train_loss: 2.9350         val_loss 2.9355 train_acc 0.2712 val_acc 0.1275\n",
      "loss 2.9269637216839253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  40%|████      | 8/20 [11:28<17:02, 85.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 008 train_loss: 2.9270         val_loss 2.9279 train_acc 0.2793 val_acc 0.1253\n",
      "loss 2.9202250831121295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  45%|████▌     | 9/20 [12:52<15:33, 84.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 009 train_loss: 2.9202         val_loss 2.9234 train_acc 0.2863 val_acc 0.1250\n",
      "loss 2.9150648637367924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  50%|█████     | 10/20 [14:19<14:14, 85.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 010 train_loss: 2.9151         val_loss 2.9185 train_acc 0.2915 val_acc 0.1254\n",
      "loss 2.909644093025011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  55%|█████▌    | 11/20 [15:41<12:42, 84.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 011 train_loss: 2.9096         val_loss 2.9150 train_acc 0.2971 val_acc 0.1222\n",
      "loss 2.905733845213733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  60%|██████    | 12/20 [17:01<11:04, 83.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 012 train_loss: 2.9057         val_loss 2.9127 train_acc 0.3011 val_acc 0.1266\n",
      "loss 2.901428609972758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  65%|██████▌   | 13/20 [18:17<09:26, 80.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 013 train_loss: 2.9014         val_loss 2.9106 train_acc 0.3057 val_acc 0.1267\n",
      "loss 2.8981465963399957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  70%|███████   | 14/20 [19:43<08:15, 82.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 014 train_loss: 2.8981         val_loss 2.9030 train_acc 0.3088 val_acc 0.1265\n",
      "loss 2.894935080661325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  75%|███████▌  | 15/20 [21:10<06:59, 83.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 015 train_loss: 2.8949         val_loss 2.9025 train_acc 0.3121 val_acc 0.1251\n",
      "loss 2.891700633077846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  80%|████████  | 16/20 [22:36<05:38, 84.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 016 train_loss: 2.8917         val_loss 2.8980 train_acc 0.3154 val_acc 0.1264\n",
      "loss 2.888791224826757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  85%|████████▌ | 17/20 [24:03<04:15, 85.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 017 train_loss: 2.8888         val_loss 2.8985 train_acc 0.3185 val_acc 0.1250\n",
      "loss 2.8865455570532785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  90%|█████████ | 18/20 [25:25<02:48, 84.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 018 train_loss: 2.8865         val_loss 2.8967 train_acc 0.3208 val_acc 0.1250\n",
      "loss 2.8842906756735327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  95%|█████████▌| 19/20 [26:45<01:22, 82.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 019 train_loss: 2.8843         val_loss 2.8947 train_acc 0.3232 val_acc 0.1251\n",
      "loss 2.882311581153485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 20/20 [28:06<00:00, 84.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 020 train_loss: 2.8823         val_loss 2.8929 train_acc 0.3252 val_acc 0.1247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN().to(device)\n",
    "\n",
    "params_count = sum(p.numel() for p in cnn.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {params_count}')\n",
    "\n",
    "history = train(model=cnn, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 294253/294253 [00:55<00:00, 5343.52it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = [], []\n",
    "for i in tqdm(range(len(test))):\n",
    "    if len(test[i]) <= MAX_WORD_LENGTH:\n",
    "        X_test.append(word_to_tensor(test[i]))\n",
    "        y_test.append(torch.tensor(i, dtype=torch.int))\n",
    "    else:\n",
    "        X_test.append(word_to_tensor(\"а\"))\n",
    "        y_test.append(torch.tensor(-1, dtype=torch.int))\n",
    "    \n",
    "X_test = torch.stack(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "precalc_res = open('precalced.txt', 'r', encoding=\"utf8\").readlines()\n",
    "for i in range(len(test)):\n",
    "    precalc_res[i] = precalc_res[i][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.eval()\n",
    "test_files = CustomDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_files, batch_size=64, shuffle=False)\n",
    "\n",
    "res = [0] * len(y_test)\n",
    "answered_count = 0\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = cnn(inputs)\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        # print(preds[:10], labels[:10])\n",
    "        \n",
    "        for prediction, pos in zip(preds, labels):\n",
    "            # print(prediction, pos)\n",
    "            if pos != -1 and precalc_res[answered_count].count('ё') == 0:\n",
    "                # print(1)\n",
    "                res[answered_count] = prediction\n",
    "            else:\n",
    "                # print(2)\n",
    "                res[answered_count] = precalc_res[answered_count]\n",
    "            answered_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn2_predictions.txt', 'w', encoding=\"utf8\") as f:\n",
    "    for i, x in enumerate(res):\n",
    "        if torch.is_tensor(x):\n",
    "            f.write(f\"{test[i][:x + 1]}^{test[i][x + 1:]}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{x}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
