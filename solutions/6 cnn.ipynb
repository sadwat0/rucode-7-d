{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTERS = \"абвгдежзийклмнопрстуфхцчшщъыьэюя\"\n",
    "VOWELS = \"аеиоуэюяы\"\n",
    "N_LETTERS = len(LETTERS)\n",
    "MAX_WORD_LENGTH = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['аа^к', 'аа^ка', 'аа^ке', 'аа^ки', 'аа^ков'],\n",
       " 495,\n",
       " ['абдоминомедиастинальный',\n",
       "  'абдоминоперикардиостомией',\n",
       "  'абдоминоперикардиостомиею',\n",
       "  'абдоминоперикардиостомии',\n",
       "  'абдоминоперикардиостомия'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = open('data/train_stresses_labels.txt', 'r', encoding=\"utf8\").readlines()\n",
    "train_word = []\n",
    "x = []\n",
    "for i in range(len(train)):\n",
    "    train[i] = train[i][:-1]\n",
    "    if len(train[i]) <= MAX_WORD_LENGTH:\n",
    "        train_word.append(train[i].replace('^', '').replace('ё', 'е'))\n",
    "    else:\n",
    "        x.append(train[i].replace('^', '').replace('ё', 'е'))\n",
    "train[:5], len(x), x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "аакам ааками ааленец аама аамами аамов аамом аамы аангичами аангичах\n"
     ]
    }
   ],
   "source": [
    "test = open('data/public_test_stresses.txt', 'r', encoding=\"utf8\").readlines()\n",
    "for i in range(len(test)):\n",
    "    test[i] = test[i][:-1].replace('ё', '')\n",
    "print(*test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_to_index(letter):\n",
    "    return LETTERS.find(letter)\n",
    "\n",
    "def letter_to_tensor(letter):\n",
    "    tensor = torch.zeros(1, N_LETTERS)\n",
    "    tensor[0][letter_to_index(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# <1 x max_word_length x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def word_to_tensor(word):\n",
    "    tensor = torch.zeros(1, MAX_WORD_LENGTH, N_LETTERS)\n",
    "    for i, letter in enumerate(word):\n",
    "        tensor[0][i][letter_to_index(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 587995/587995 [01:53<00:00, 5194.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = [], []\n",
    "for i in tqdm(range(len(train_word))):\n",
    "    if len(train[i]) <= MAX_WORD_LENGTH:\n",
    "        X.append(word_to_tensor(train_word[i]))\n",
    "        correct_pos = train[i].find('^')\n",
    "        y.append(torch.zeros(MAX_WORD_LENGTH))\n",
    "        y[-1][correct_pos] = 1\n",
    "X = torch.stack(X)\n",
    "y = torch.stack(y)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_tensor, target_tensor):\n",
    "        self.data = data_tensor\n",
    "        self.target = target_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.target[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = CustomDataset(X_train, y_train)\n",
    "val_files = CustomDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # 1 x 23 x 32\n",
    "            nn.Conv2d(1, 8, kernel_size=3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            \n",
    "            # 1 x 21 x 30\n",
    "            nn.Conv2d(8, 16, kernel_size=3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # 16 x 9 x 14\n",
    "            nn.Conv2d(16, 32, kernel_size=3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # 32 x 7 x 12\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # 64 x 5 x 10\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # 64 x 3 x 8\n",
    "            nn.Conv2d(64, 32, kernel_size=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(0.2)\n",
    "            \n",
    "            # 32 x 2 x 7\n",
    "        )\n",
    "        \n",
    "        self.fully_conected = nn.Sequential(\n",
    "            nn.Linear(448, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            # nn.Dropout(0.25),\n",
    "            nn.BatchNorm1d(256),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            # nn.Dropout(0.25),\n",
    "            nn.BatchNorm1d(512),\n",
    "            \n",
    "            # nn.Linear(512, 128),\n",
    "            # nn.LeakyReLU(),\n",
    "            # # nn.Dropout(0.25),\n",
    "            # nn.BatchNorm1d(128),\n",
    "            \n",
    "            # nn.Linear(128, 64),\n",
    "            # nn.LeakyReLU(),\n",
    "            # # nn.Dropout(0.25),\n",
    "            # nn.BatchNorm1d(64),\n",
    "            \n",
    "            nn.Linear(512, MAX_WORD_LENGTH),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        # print(x.shape)\n",
    "        \n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.fully_conected(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_epoch(model, train_loader, criterion, optimizer):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_data = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        # print(outputs.shape, labels[0])\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == torch.argmax(labels.data, 1))\n",
    "        processed_data += inputs.size(0)\n",
    "              \n",
    "    train_loss = running_loss / processed_data\n",
    "    train_acc = running_corrects.cpu().numpy() / processed_data\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_size = 0\n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == torch.argmax(labels.data))\n",
    "        processed_size += inputs.size(0)\n",
    "    val_loss = running_loss / processed_size\n",
    "    val_acc = running_corrects.double() / processed_size\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, batch_size):\n",
    "    train_loader = DataLoader(train_files, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_files, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
    "        val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
    "    \n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "        # opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        opt = torch.optim.AdamW(model.parameters())\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=opt, gamma=0.9)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n",
    "            print(\"loss\", train_loss)\n",
    "            \n",
    "            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
    "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
    "            \n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
    "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 329831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|          | 0/20 [00:00<?, ?it/s]c:\\Users\\Ivan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 3.040870185943894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   5%|▌         | 1/20 [01:40<31:57, 100.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 001 train_loss: 3.0409         val_loss 3.0145 train_acc 0.1608 val_acc 0.1395\n",
      "loss 3.0075056600141363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  10%|█         | 2/20 [03:08<27:51, 92.88s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 002 train_loss: 3.0075         val_loss 3.0008 train_acc 0.1966 val_acc 0.1398\n",
      "loss 2.9840349384675235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  15%|█▌        | 3/20 [04:35<25:35, 90.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 003 train_loss: 2.9840         val_loss 2.9672 train_acc 0.2211 val_acc 0.1355\n",
      "loss 2.953312252727308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  20%|██        | 4/20 [06:02<23:43, 88.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 004 train_loss: 2.9533         val_loss 2.9437 train_acc 0.2529 val_acc 0.1339\n",
      "loss 2.932711900154874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  25%|██▌       | 5/20 [07:29<22:03, 88.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 005 train_loss: 2.9327         val_loss 2.9269 train_acc 0.2737 val_acc 0.1325\n",
      "loss 2.915481936874475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  30%|███       | 6/20 [08:56<20:30, 87.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 006 train_loss: 2.9155         val_loss 2.9060 train_acc 0.2912 val_acc 0.1306\n",
      "loss 2.889054240895065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  35%|███▌      | 7/20 [10:23<18:57, 87.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 007 train_loss: 2.8891         val_loss 2.8893 train_acc 0.3179 val_acc 0.1268\n",
      "loss 2.8711840747295008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  40%|████      | 8/20 [11:50<17:28, 87.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 008 train_loss: 2.8712         val_loss 2.8728 train_acc 0.3363 val_acc 0.1275\n",
      "loss 2.8575099316352426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  45%|████▌     | 9/20 [13:19<16:08, 88.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 009 train_loss: 2.8575         val_loss 2.8629 train_acc 0.3502 val_acc 0.1277\n",
      "loss 2.844894647101032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  50%|█████     | 10/20 [14:46<14:35, 87.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 010 train_loss: 2.8449         val_loss 2.8585 train_acc 0.3631 val_acc 0.1296\n",
      "loss 2.8338582280456954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  55%|█████▌    | 11/20 [16:12<13:03, 87.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 011 train_loss: 2.8339         val_loss 2.8451 train_acc 0.3741 val_acc 0.1268\n",
      "loss 2.8243612945355867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  60%|██████    | 12/20 [17:37<11:32, 86.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 012 train_loss: 2.8244         val_loss 2.8401 train_acc 0.3839 val_acc 0.1277\n",
      "loss 2.8163941994208903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  65%|██████▌   | 13/20 [19:03<10:03, 86.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 013 train_loss: 2.8164         val_loss 2.8318 train_acc 0.3919 val_acc 0.1278\n",
      "loss 2.8093066807087967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  70%|███████   | 14/20 [20:31<08:41, 86.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 014 train_loss: 2.8093         val_loss 2.8260 train_acc 0.3993 val_acc 0.1275\n",
      "loss 2.8027984217115107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  75%|███████▌  | 15/20 [22:00<07:17, 87.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 015 train_loss: 2.8028         val_loss 2.8232 train_acc 0.4058 val_acc 0.1271\n",
      "loss 2.7972074117662657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  80%|████████  | 16/20 [23:28<05:50, 87.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 016 train_loss: 2.7972         val_loss 2.8199 train_acc 0.4114 val_acc 0.1267\n",
      "loss 2.7923147620449944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  85%|████████▌ | 17/20 [24:55<04:22, 87.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 017 train_loss: 2.7923         val_loss 2.8172 train_acc 0.4164 val_acc 0.1264\n",
      "loss 2.7880024643303165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  90%|█████████ | 18/20 [26:23<02:55, 87.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 018 train_loss: 2.7880         val_loss 2.8142 train_acc 0.4207 val_acc 0.1270\n",
      "loss 2.783861698220692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  95%|█████████▌| 19/20 [27:53<01:28, 88.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 019 train_loss: 2.7839         val_loss 2.8106 train_acc 0.4248 val_acc 0.1274\n",
      "loss 2.7798562160590192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 20/20 [29:22<00:00, 88.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 020 train_loss: 2.7799         val_loss 2.8097 train_acc 0.4290 val_acc 0.1272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN().to(device)\n",
    "\n",
    "params_count = sum(p.numel() for p in cnn.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {params_count}')\n",
    "\n",
    "history = train(model=cnn, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 294253/294253 [00:54<00:00, 5413.61it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = [], []\n",
    "for i in tqdm(range(len(test))):\n",
    "    if len(test[i]) <= MAX_WORD_LENGTH:\n",
    "        X_test.append(word_to_tensor(test[i]))\n",
    "        y_test.append(torch.tensor(i, dtype=torch.int))\n",
    "    else:\n",
    "        X_test.append(word_to_tensor(\"а\"))\n",
    "        y_test.append(torch.tensor(-1, dtype=torch.int))\n",
    "    \n",
    "X_test = torch.stack(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "precalc_res = open('precalced.txt', 'r', encoding=\"utf8\").readlines()\n",
    "for i in range(len(test)):\n",
    "    precalc_res[i] = precalc_res[i][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.eval()\n",
    "test_files = CustomDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_files, batch_size=64, shuffle=False)\n",
    "\n",
    "res = [0] * len(y_test)\n",
    "answered_count = 0\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = cnn(inputs)\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        # print(preds[:10], labels[:10])\n",
    "        \n",
    "        for prediction, pos in zip(preds, labels):\n",
    "            # print(prediction, pos)\n",
    "            if pos != -1 and precalc_res[answered_count].count('ё') == 0:\n",
    "                # print(1)\n",
    "                res[answered_count] = prediction\n",
    "            else:\n",
    "                # print(2)\n",
    "                res[answered_count] = precalc_res[answered_count]\n",
    "            answered_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn_predictions.txt', 'w', encoding=\"utf8\") as f:\n",
    "    for i, x in enumerate(res):\n",
    "        if torch.is_tensor(x):\n",
    "            f.write(f\"{test[i][:x + 1]}^{test[i][x + 1:]}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{x}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
